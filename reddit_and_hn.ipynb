{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing subreddit: classicwow\n",
      "Successfully scraped posts and comments from classicwow.\n",
      "Accessing subreddit: ArtificialInteligence\n",
      "Successfully scraped posts and comments from ArtificialInteligence.\n",
      "Accessing subreddit: europe\n",
      "Successfully scraped posts and comments from europe.\n",
      "Accessing subreddit: lifehacks\n",
      "Successfully scraped posts and comments from lifehacks.\n",
      "Accessing subreddit: Millennials\n",
      "Successfully scraped posts and comments from Millennials.\n",
      "Accessing subreddit: OculusQuest\n",
      "Successfully scraped posts and comments from OculusQuest.\n",
      "Accessing subreddit: instacart\n",
      "Successfully scraped posts and comments from instacart.\n",
      "Accessing subreddit: PetPeeves\n",
      "Successfully scraped posts and comments from PetPeeves.\n",
      "Accessing subreddit: nvidia\n",
      "Successfully scraped posts and comments from nvidia.\n",
      "Accessing subreddit: LifeProTips\n",
      "Successfully scraped posts and comments from LifeProTips.\n",
      "Accessing subreddit: mathmemes\n",
      "Successfully scraped posts and comments from mathmemes.\n",
      "Accessing subreddit: interestingasfuck\n",
      "Successfully scraped posts and comments from interestingasfuck.\n",
      "Accessing subreddit: amazonprime\n",
      "Successfully scraped posts and comments from amazonprime.\n",
      "Accessing subreddit: doordash\n",
      "Successfully scraped posts and comments from doordash.\n",
      "Accessing subreddit: Amd\n",
      "Successfully scraped posts and comments from Amd.\n",
      "Accessing subreddit: BrandNewSentence\n",
      "Successfully scraped posts and comments from BrandNewSentence.\n",
      "Accessing subreddit: HobbyDrama\n",
      "Successfully scraped posts and comments from HobbyDrama.\n",
      "Accessing subreddit: Android\n",
      "Successfully scraped posts and comments from Android.\n",
      "Accessing subreddit: AskReddit\n",
      "Successfully scraped posts and comments from AskReddit.\n",
      "Accessing subreddit: technology\n",
      "Successfully scraped posts and comments from technology.\n",
      "Data saved to reddit_posts_1.xlsx.\n",
      "                                                  title  \\\n",
      "0     Don't be afraid to leave your guild if you bel...   \n",
      "1     Don't be afraid to leave your guild if you bel...   \n",
      "2     Don't be afraid to leave your guild if you bel...   \n",
      "3     Has anyone started a successful investment fun...   \n",
      "4     Has anyone started a successful investment fun...   \n",
      "...                                                 ...   \n",
      "5625  TikTok users flock to Chinese app RedNote befo...   \n",
      "5626  TikTok users flock to Chinese app RedNote befo...   \n",
      "5627  TikTok users flock to Chinese app RedNote befo...   \n",
      "5628  TikTok users flock to Chinese app RedNote befo...   \n",
      "5629  TikTok users flock to Chinese app RedNote befo...   \n",
      "\n",
      "                                                    url  score       created  \\\n",
      "0     https://www.reddit.com/r/classicwow/comments/1...      2  1.736860e+09   \n",
      "1     https://www.reddit.com/r/classicwow/comments/1...      2  1.736860e+09   \n",
      "2     https://www.reddit.com/r/classicwow/comments/1...      2  1.736860e+09   \n",
      "3     https://www.reddit.com/r/classicwow/comments/1...      0  1.736860e+09   \n",
      "4     https://www.reddit.com/r/classicwow/comments/1...      0  1.736860e+09   \n",
      "...                                                 ...    ...           ...   \n",
      "5625     https://www.bbc.com/news/articles/c2475l7zpqyo      0  1.736852e+09   \n",
      "5626     https://www.bbc.com/news/articles/c2475l7zpqyo      0  1.736852e+09   \n",
      "5627     https://www.bbc.com/news/articles/c2475l7zpqyo      0  1.736852e+09   \n",
      "5628     https://www.bbc.com/news/articles/c2475l7zpqyo      0  1.736852e+09   \n",
      "5629     https://www.bbc.com/news/articles/c2475l7zpqyo      0  1.736852e+09   \n",
      "\n",
      "       subreddit                                 submission_content  \\\n",
      "0     classicwow  So I was raiding with this group for a few wee...   \n",
      "1     classicwow  So I was raiding with this group for a few wee...   \n",
      "2     classicwow  So I was raiding with this group for a few wee...   \n",
      "3     classicwow  I am new to WoW and was surprised to see that ...   \n",
      "4     classicwow  I am new to WoW and was surprised to see that ...   \n",
      "...          ...                                                ...   \n",
      "5625  technology                                                      \n",
      "5626  technology                                                      \n",
      "5627  technology                                                      \n",
      "5628  technology                                                      \n",
      "5629  technology                                                      \n",
      "\n",
      "                                                comment        comment_author  \n",
      "0                                       Name and shame!               zSHARPz  \n",
      "1     Don't think you can on this sub it's against t...                Magnon  \n",
      "2              Good way to get banned from this sub lol  MidnightFireHuntress  \n",
      "3                             Sounds like a total scam.  MidnightFireHuntress  \n",
      "4     Why should it be a scam? The model is well exp...    Mean_Education_174  \n",
      "...                                                 ...                   ...  \n",
      "5625  So then eventually RedNote will be banned as w...            JediForces  \n",
      "5626                                       What damage?             nicuramar  \n",
      "5627                                tiktok is done for.               Lex2882  \n",
      "5628                              So no damage then lol            JediForces  \n",
      "5629                 Flocking to rednote won't do much.               Lex2882  \n",
      "\n",
      "[5630 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='',         # Replace with your client ID\n",
    "    client_secret='', # Replace with your client secret\n",
    "    user_agent=''      # Replace with a user agent (a string)\n",
    ")\n",
    "\n",
    "\n",
    "# Keywords to search for subreddits\n",
    "keywords = ['customer service automation','GenAI in contact center','customer service','generative ai']\n",
    "posts = []\n",
    "\n",
    "def search_subreddits(keywords):\n",
    "    \"\"\"Search for subreddits that match the given keywords.\"\"\"\n",
    "    subreddit_list = set()\n",
    "    for keyword in keywords:\n",
    "        for submission in reddit.subreddit('all').search(keyword, limit=5):  # Limit to avoid excessive API calls\n",
    "            subreddit_list.add(submission.subreddit.display_name)\n",
    "    return list(subreddit_list)\n",
    "\n",
    "def is_valid_subreddit(subreddit):\n",
    "    \"\"\"Check if a subreddit exists and is public.\"\"\"\n",
    "    try:\n",
    "        subreddit_instance = reddit.subreddit(subreddit)\n",
    "        # Check if the subreddit is private or doesn't exist\n",
    "        return not subreddit_instance.over18 and subreddit_instance.display_name is not None\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking subreddit {subreddit}: {e}\")\n",
    "        return False\n",
    "\n",
    "def scrape_subreddit(subreddit):\n",
    "    \"\"\"Scrape all posts and comments from a valid subreddit.\"\"\"\n",
    "    print(f\"Accessing subreddit: {subreddit}\")\n",
    "    try:\n",
    "        # Attempt to fetch new submissions\n",
    "        for submission in reddit.subreddit(subreddit).new(limit=10):  # Adjust limit as needed\n",
    "            submission.comments.replace_more(limit=0)  # Fetch all comments\n",
    "            for comment in submission.comments.list():\n",
    "                posts.append({\n",
    "                    'title': submission.title,\n",
    "                    'url': submission.url,\n",
    "                    'score': submission.score,\n",
    "                    'created': submission.created_utc,\n",
    "                    'subreddit': subreddit,\n",
    "                    'submission_content': submission.selftext,\n",
    "                    'comment': comment.body,\n",
    "                    'comment_author': comment.author.name if comment.author else \"Unknown\"\n",
    "                })\n",
    "        print(f\"Successfully scraped posts and comments from {subreddit}.\")\n",
    "    except praw.exceptions.RedditAPIException as api_error:\n",
    "        print(f\"Reddit API error while accessing {subreddit}: {api_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while accessing {subreddit}: {e}\")\n",
    "\n",
    "# Search for relevant subreddits\n",
    "found_subreddits = search_subreddits(keywords)\n",
    "\n",
    "# Scraping Reddit\n",
    "for subreddit in found_subreddits:\n",
    "    if is_valid_subreddit(subreddit):\n",
    "        scrape_subreddit(subreddit)\n",
    "    else:\n",
    "        print(f\"Subreddit {subreddit} is invalid or inaccessible.\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "reddit_df = pd.DataFrame(posts)\n",
    "\n",
    "# Store results in an Excel file\n",
    "output_file = 'reddit_posts_1.xlsx'\n",
    "reddit_df.to_excel(output_file, index=False)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Data saved to {output_file}.\")\n",
    "print(reddit_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to hacker_news_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# List of keywords to search\n",
    "keywords = [\"customer service\", \"machine learning\", \"generative ai\", \"chatbot services\"]\n",
    "\n",
    "# Function to extract the required fields from each item in the data\n",
    "def extract_news_data(data):\n",
    "    news_data = []\n",
    "    try:\n",
    "        for item in data['hits']:\n",
    "            # Use .get() to safely access fields that might be missing\n",
    "            title = item.get('title', 'No Title Available')\n",
    "            story_date = item.get('created_at', 'No Date Available')\n",
    "            author = item.get('author', 'Unknown Author')\n",
    "            url = item.get('url', 'No URL Available')\n",
    "            \n",
    "            # Get matched keywords and full text with highlights removed, with safe access\n",
    "            matched_keywords = item.get('_highlightResult', {}).get('title', {}).get('matchedWords', [])\n",
    "            news_text = item.get('_highlightResult', {}).get('title', {}).get('value', '').replace(\"<em>\", \"\").replace(\"</em>\", \"\")\n",
    "\n",
    "            # Append the extracted data as a dictionary to the list\n",
    "            news_data.append({\n",
    "                \"News Title\": title,\n",
    "                \"News Text\": news_text,\n",
    "                \"Story Date\": story_date,\n",
    "                \"Matched Keywords\": \", \".join(matched_keywords),\n",
    "                \"URL\": url,\n",
    "                \"Author Name\": author\n",
    "            })\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    return news_data\n",
    "\n",
    "# Collect data from the API for each keyword\n",
    "final_news_data = []\n",
    "for keyword in keywords:\n",
    "    url = f\"https://hn.algolia.com/api/v1/search?query={keyword}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Append extracted data to final_news_data\n",
    "        final_news_data.extend(extract_news_data(data))\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for keyword: {keyword}\")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(final_news_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = \"hacker_news_search_results.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
